:toc: macro
:toc-title:
:toclevels: 9

= gta3sc Design Internals

toc::[]

== Introduction

This document describes the design of the gta3sc project. It captures a high-level overview of the infrastructure and reasons about some of the design choices behind it.

== Design Goals

The current codebase is a complete rewrite of a previous one. Clear goals are made in order to avoid the same mistakes as before.

The project has four design goals and many decisions are based upon them:

 * The code base should be **simple**: There should be no magic tricks. Triviality should be chosen whenever possible. Understanding the code base quickly is a must.
 * The code base should be **modular**: Each module should work as a simple "program" in charge of a very specific task. Modules should be easily replaceable. Understanding a single module should be enough for hacking on it.
 * The code base should be **efficient**: Asymptotic complexity matters. Cache hits too. Sources of unpredictable performance (such as memory allocations) should be avoided. Data structures should be designed in a way that keeps the cache hot.
 * The translator should be **conformant**: Whereas the authoritative source is https://github.com/GTAmodding/gta3script-specs[gta3script-specs].

== Language Conformance

The original language is very hard to get right. There are many hidden tricky parts. This observation comes from the previous codebase not handling many of them. 

It was clear a formal specification had to be written before touching code. The big picture had to be seen in order to understand and handle the corner cases.

As this is a compiler infrastructure, not a language-wide repository, a separate project for reversing the language and formalizing it has been created. It can be found at https://github.com/GTAmodding/gta3script-specs[GTAModding/gta3script-specs].

== Basic Library

=== Source Mapping

TODO

=== Diagnostic Handling

TODO

=== Command Information

TODO

=== Object Information

TODO


== Compiler

=== Overview

+++
<p align="center">
<img src="https://public.thelink2012.xyz/gta3sc/pipeline.svg?v2" alt="gta3sc pipeline">
<img src="https://public.thelink2012.xyz/gta3sc/pipeline-miss2.svg" alt="miss2 pipeline">
</p>
+++

_The architecture of gta3sc (on the left) is very different from the original DMA Design compiler (on the right)._

The https://en.wikipedia.org/wiki/Rockstar_North[DMA Design] compiler (hereafter https://public.thelink2012.xyz/gta3/miss2_v413.zip[miss2]) is a scannerless parser followed by multiple lowering passes. Semantic checks are performed every phase as more information is gathered.

The diagram on the right is a oversimplification of miss2. In reality, the compiler phases are all over the place. It is enough for in-house usage, but the same design cannot be used in name of maintenance and extensibility.

Instead, a different architecture has been designed. Each phase deals with a single problem and no mystic hacks are performed.

Each script needs to run under a parsing pass whereas the source text is transformed into a intermediate representation. This pass consists of three phases: preprocessing, scanning and parsing.

The preprocessor performs normalization of the input text. This includes removal of comments and duplicated whitespaces.

The scanner transforms the stream of characters given by the preprocessor into a stream of tokens.

The parser takes the stream of tokens and verifies whether it forms a syntactically valid script. In the process, it builds an intermediate representation for further processing by other passes.

The scanner and the preprocessor are streams. Therefore, the three phases are run concurrently. The parser asks the scanner for the next token. The scanner asks the preprocessor for the next characters. This is very beneficial. The preprocessor and scanner, despite introducing two phases, do not bring in any additional overhead.

The IR produced by the parser does not carry any semantic information. Semantic checking and annotation of the IR with this information is a task for the next phase.

The semantic analyzer requires to see the full content of all scripts in a very specific order. Thus, the IR produced by the parsing of each script is merged into a single big IR which can be used for semantic checking.

The semantic phase takes the IR produced by the parsers, validates its semantics, and produces another IR with semantic information embedded in.

Afterwards, the IR goes into a transformation pass called lowering. This pass rewrites complex constructs (such as IF and REPEAT) in terms of simpler ones (labels and jumps). This way, further passes only need to deal with simple commands.

The final phase is code generation. It takes the IR and produces bytecode for the execution environment.

=== Preprocessor

The preprocessor's job is to normalize the character stream of the source code.

It strips comments and blank characters from the beginning and end of each line, as well as transforms multiple whitespaces into a single one. CRLF to LF normalization is also performed.

Notice whitespaces are not ignored in this language. So normalizing them is very important.

This step could be in the scanner, but given the specifics of the language, it would cause lexing to become too complicated. The scanner would leave the scope of regular languages and go far beyond (e.g. nested block comments are context free).

Another scenario that would work against the scanner is, for instance, when there is an interleaving sequence of whitespaces and block comments preceding the first word in a line. Handling this requires too many state changes, resulting in spaghetti code.

This phase is analogous to the line reconstruction phase of miss2, except it is capable of doing so character by character.

=== Scanner

The scanner takes the normalized stream of characters produced by the preprocessor and classifies them into tokens.

The GTA3script language is itself context sensitive. In order for the scanner to be simple and regular, it must perform a conservative form of classification. Unlike in other languages, integers, floats and identifiers are not classified individually, instead they are classified into a generic category called _word_.

Later on the parser has enough contextual information to disambiguate whether this _word_ is a command, label, integer, float or identifier.

There is a kind of token that cannot be classified by the main automata. The _filename_. Such token may contain arithmetic characters (e.g. `file-name.sc`) in it, which usually would mean three separate tokens. The scanner classifies filenames only upon explicit request by the parser.

The other lexical categories are _whitespace_, _end of line_, _string_, and  one for each _arithmetic operator_.

The motivation for having a scanning phase is that it severely simplifies the parser. The process of looking ahead becomes much easier. Even the scannerless miss2 have a scanner hidden in its parsing cruft by the need to lookahead during expression parsing.

=== Parser

The parser checks the syntactical validity of the stream of tokens produced by the scanner and constructs an intermediate representation of the language in the process.

The grammar and more details about the language itself can be found at the https://github.com/GTAmodding/gta3script-specs[language specification].

The intermediate representation is not an abstract syntax tree, but a linear code. The properties of the language do not call for an AST. There is no nested expressions, for instance, and control-flow structures can be easily represented as code.

The IR is guaranted to be syntactically valid, but semantic validation is left to its own phase. The parser does not know anything about commands, alternators and types. Please see the section on <<ParserIR, Parser IR>> for the properties of this representation.

The entire IR shares the same lifetime. Consequently it benefits from the use of a <<ArenaMemoryResource,region-based memory manager>>. This mechanism guarantees us lightning fast allocation/deallocation and very good cache locality.

The parser does not perform syntax-directed translation, which would make parsing and semantic checking a single pass. This is not possible because the semantic phase requires global information about the program. First, it needs to know the name of every symbol in the entire program before checking semantics. And second, the entity a variable holds depends on previous assignments, and the definition of _previous_ require a total ordering of the lines of code. More details about this can be seen on the language specification.

This pass also populates the <<SymbolRepository,symbol repository>> with the filename of scripts required by this stream of tokens.

Addition of new features to the language only touches this phase onwards, specially because the scanner is conservative and produces no keywords.

=== Semantics

The semantic phase checks the semantic validity of the IR produced by the parser and, in the process, produces another IR guaranted to be semantically valid.

This phase is divided into two passes. The first pass discovers declarations in the IR (e.g. variables and labels). The second pass performs semantic validation on each and every command.

There is not much else to say. This phase is quite obvious and follows the language specification. Please see the section on <<SemaIR, Semantic IR>> for the properties of the output of the phase. Much like the parser, the resulting IR is constructed in an <<ArenaMemoryResource,arena>> (which does not need to be the same as the parser's arena).

This phase also populates the <<SymbolRepository,symbol repository>> with variables, labels, user-defined string constants, used objects, and more.


=== Lowering Pass

The lowering pass takes an IR and transforms its complex commands into simpler commands, in such a way that further passes over the IR are simplified.

The first class of complex commands that are simplified is the class of selection and iteration statements (`IF`, `WHILE`, `REPEAT` and similar). These are rewritten in terms of branch commands (`ANDOR`, `GOTO_IF_FALSE`, `GOTO_IF_TRUE`).

The second class of simplifications are on require statements. Filenames are converted into labels (for `GOSUB_FILE`, `LAUNCH_MISSION`, `LOAD_AND_LAUNCH_MISSION`).

=== Code Generation

The code generator takes IR and emits equivalent bytecode into a byte stream.

The generation occurs in a single pass over the IR and a second fixup pass over certain locations in the byte stream.

The first pass reserves space for the header in the stream and proceeds to emit the bytecode for each command in the IR. In the process, it takes note of some key information to be stored in the header, such as used objects and script sizes.

Forward references to labels and variables do not emit bytecode. Instead, it reserves space for the label (or variable) offset and stores the location of the reserved space in a fixup table. Backward references (those happening after declaration) do not need fixup entries.

The second pass fills the header and patches all the references in the fixup table.

During emission, the generator discards a few commands (e.g. `NOP`, `VAR_INT`, `{`, `}`) and translates others into their internal equivalents (e.g. `LOAD_AND_LAUNCH_MISSION` into `LOAD_AND_LAUNCH_MISSION_INTERNAL`).

== Compiler Driver

TODO

== Support Library

[#ArenaMemoryResource]
=== Memory Arena

TODO

== Data Structures

[#ParserIR]
=== Parser IR

//image:https://public.thelink2012.xyz/gta3sc/parser-ir.svg[role="related left", width="250"]
++++
<img style="margin-right: .625em;" role="left" align="left" src="https://public.thelink2012.xyz/gta3sc/parser-ir.svg" width="250">
++++

The intermediate representation generated by the parser is a linear code whereas each instruction is a tuple _(label, command, argument...)_.

Both label and commands may be null, and the same label name may appear more than once.

The intermediate language is very similar to GTA3script itself, with few little changes. These changes are better detailed in the _Code Shape_ section of the language specification.

Since the parser does not know anything about commands and its parameters, command names are directly stored in the IR and the commands themselves may not exist. The amount of arguments may be wrong (e.g. three arguments in an `ADD_SCORE` instead of two). The typing may also be wrong (e.g. instead of a integer argument for `WAIT`, a floating one is given).

This property extends to all other commands, even special ones like `VAR_INT`, `REPEAT` and `{`. Commands generated by the parsing process itself (e.g. `IF 0`) should not be trusted either.

It is guaranteed, however, that control-flow and scoping instructions are properly matched (e.g. for every `IF` there is an `ENDIF`).

The type of an arguments is either an _integer_, _float_, _string_, _identifier_ or _filename_. Identifiers are not yet resolved. Their names are guaranteed to be syntactically valid but the semantic phase have to decide whether it is a variable, label, string identifier or string constant.

Information about the location in the source code that originated each element is maintained.

++++
<div style="clear:both"></div>
++++

[#SemaIR]
=== Semantic IR

TODO

[#SymbolRepository]
=== Symbol Repository

TODO

